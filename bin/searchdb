#!/usr/bin/env python3
"""
Extracts details about each document and adds them to a database.
"""

import pathlib
import sqlite3
import bs4

def get_summary(soup):
    """Extract a summary from the HTML content."""
    summary = ""
    p_elts = soup.find_all("p", class_=False)
    for p in p_elts:
        p = p.text
        # If the body isn't empty and longer than 50 characters (arbitrary)
        if p.strip() and len(p) > 50:
            # Limit summary to 250 characters (including truncation)
            summary = p.strip()[0:247]

            # Replace newlines to format query string
            summary = summary.replace("\n", " ")

            # Truncate endings
            summary = summary + "..."
            break

    return summary


def build_search_db(input_dir, output_db_path):
    """
    Build the search database from inverted index files.

    Args:
        input_dir (pathlib.Path): Directory containing inverted index files.
        output_db_path (pathlib.Path): Path to the output SQLite database.
    """
    conn = sqlite3.connect(output_db_path)
    c = conn.cursor()

    # Create table
    c.execute("""
        CREATE TABLE IF NOT EXISTS documents (
            docid INTEGER PRIMARY KEY,
            title VARCHAR(150),
            summary VARCHAR(250),
            url VARCHAR(150)
        )
    """)

    # Process each crawl HTML file
    for index_file in sorted(input_dir.glob("*.html")):
        with index_file.open("r", encoding="utf-8") as f:
            soup = bs4.BeautifulSoup(f.read(), "html.parser")

        # docid
        meta_docid = soup.find("meta", attrs={"eecs485_docid": True})
        if meta_docid is None or not meta_docid.has_attr("eecs485_docid"):
            # logging.warning("Skipping %s: missing eecs485_docid meta", index_file)
            # skipped += 1
            continue
        
        docid = int(meta_docid.get("eecs485_docid"))

        # title
        title_tag = soup.find("title")
        if title_tag and title_tag.string:
            title = title_tag.text.strip()
        if title.endswith(" - Wikipedia"):
            title = title[:-len(" - Wikipedia")]

        # summary
        summary = get_summary(soup)

        # url
        meta_url = soup.find("meta", attrs={"eecs485_url": True})
        if meta_url and meta_url.has_attr("eecs485_url"):
            url = meta_url.get("eecs485_url").strip()[:150]
        else:
            url = ""

        # Insert row
        c.execute("""
            INSERT OR REPLACE INTO documents (docid, title, summary, url)
            VALUES (?, ?, ?, ?)
        """, (docid, title, summary, url))

    conn.commit()
    conn.close()


def main():
    """Main function to run the searchdb script."""

    # set paths
    input_dir = pathlib.Path("inverted_index/crawl/")
    output_dir = pathlib.Path("var/search.sqlite3")

    # create output directory if it doesn't exist
    output_dir.parent.mkdir(parents=True, exist_ok=True)

    # remove existing database if it exists
    if output_dir.exists():
        output_dir.unlink()

    build_search_db(input_dir, output_dir)

if __name__ == "__main__":
    main()
